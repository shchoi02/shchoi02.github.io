---
title: Deep Learning Start
date: 2025-07-24 13:40:20 +09:00
categories: ['deep learning']
tags: ['deep learning', 'training', 'loss function']
---

> 출처: 사이토 고키, 『밑바닥부터 시작하는 딥러닝』 (2017)  
> 본 문서는 **4장: 신경망 학습**의 내용을 정리한 것입니다.

딥러닝 모델을 사용하려면 반드시 **학습** 과정을 거쳐야 합니다.  
학습이란 **훈련 데이터로부터 가중치와 편향 같은 매개변수의 최적값을 자동으로 찾아내는 과정**을 의미합니다.  
이때 모델이 올바르게 학습하도록 이끌어주는 것이 바로 **손실 함수(loss function)** 입니다.

손실 함수는 모델이 얼마나 잘못했는지를 수치로 보여주며, 이 값을 줄이는 방향으로 parameter를 update합니다.  
학습 = 손실 함수를 줄이는 과정입니다.

이번 장에서는 손실 함수에 들어가기 전까지, **신경망이 어떻게 데이터에서 학습하는지, 그리고 왜 훈련 데이터와 시험 데이터를 나누는지**를 살펴봅니다.

## 4.1 데이터에서 학습

### 4.1.1 데이터 주도 학습

- 기계학습의 핵심은 **데이터**입니다.
- 전통적인 접근: 사람이 직접 특징(feature)을 설계 → 기계가 그 특징으로 학습
  - 예: SIFT, SURF, HOG 같은 특징 추출 + SVM, KNN 분류기
- 신경망 접근: 데이터를 있는 그대로 받아서 **특징 추출 + 학습을 동시에 수행**
  - 신경망이 **중요한 특징까지 자동으로 학습**
- 이런 방식을 **end-to-end 학습**이라고 합니다.

👉 장점: 문제마다 일일이 특징을 정의할 필요 없이, 다양한 문제를 같은 방식으로 풀 수 있습니다.

---

### 4.1.2 훈련 데이터와 시험 데이터

- 기계학습에서는 데이터를 두 부분으로 나눕니다:
  1. **훈련 데이터(training set):** 매개변수 학습에 사용
  2. **시험 데이터(test set):** 모델의 범용 능력 평가에 사용

- **범용 능력(generalization):**  
  → 한 번도 보지 못한 새로운 데이터도 잘 풀어내는 능력

- **왜 분리해야 할까?**
  - 훈련 데이터로만 평가하면 모델이 데이터를 "외워버리는" 현상이 발생
  - 이렇게 특정 데이터셋에만 과도하게 맞춰진 상태를 **과적합(overfitting)** 이라고 부름

👉 따라서 좋은 학습은 **훈련 데이터에는 잘 맞으면서, 시험 데이터에서도 높은 성능**을 내야 합니다.

---

## 4.2 손실 함수

- 학습의 목표는 **손실 함수(loss function)를 최소화하는 것**입니다.
- 손실 함수는 현재 모델이 얼마나 잘못했는지를 수치로 표현합니다.

**정확도(accuracy) 대신 손실 함수를 쓰는 이유**
- 정확도는 값이 이산적(discrete)이라서 미분 불가능 → 경사하강법을 쓸 수 없음
- 손실 함수는 연속적이고 미분 가능 → 매개변수를 효율적으로 갱신할 수 있음

👉 정리: 신경망 학습은 손실 함수를 정의하고, 이를 최소화하는 방향으로 매개변수를 조정하는 과정입니다.

### 4.2.1 손실 함수의 종류 

손실 함수는 크게 두 가지 유형으로 나뉩니다.
+ 회귀(regression) 문제: 예측값이 연속값일 때 (예: 집값 예측) → 평균제곱오차(MSE), 평균절대오차(MAE)
+ 분류(classification) 문제: 예측값이 클래스일 때 (예: 숫자 분류) → 교차 엔트로피 오차(CE), Hinge Loss 등

대규모 언어모델(LLM)도 **분류(classification)** 문제로 이해할 수 있습니다.
- 입력: 지금까지의 문맥(토큰 시퀀스)
- 출력: 수만~수십만 단어 중 **다음에 올 단어 하나** 선택 → 다중 클래스 분류 문제
- 손실 함수: **Cross-Entropy Loss**
- 평가 지표: **Perplexity (평균 손실의 지수)**

거대한 규모로 확장된 "단어 맞히기 분류 문제"가 바로 LLM 학습의 본질입니다.

## 맺음말

정리하자면, **신경망 학습의 본질은 데이터를 통해 스스로 패턴을 찾는 과정**입니다.  
사람이 일일이 규칙을 설계하는 것이 아니라, 신경망은 데이터에서 직접 중요한 특징을 학습합니다.  
그러나 단순히 훈련 데이터에만 잘 맞는 모델은 쓸모가 없습니다.  
새로운 데이터에서도 성능을 발휘해야 하고, 이를 위해 **훈련 데이터와 시험 데이터를 분리**해야 합니다.

그리고 이 모든 학습 과정의 중심에는 **손실 함수**가 있습니다.  
손실 함수는 모델이 틀린 정도를 수치화하여 "어느 방향으로 개선해야 하는지"를 알려줍니다.  
이 손실 함수를 경사하강법 같은 최적화 기법으로 줄여 나가면서, 신경망은 점점 더 나은 성능을 얻게 됩니다.

> 쉽게 말해,
> **훈련 데이터**는 신경망을 가르치는 교재이고,
> **시험 데이터**는 그 실력을 점검하는 시험지이며,
> **손실 함수**는 공부를 얼마나 잘했는지 알려주는 점수표라고 할 수 있습니다.

